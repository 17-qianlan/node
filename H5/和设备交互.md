f## H5核心API
### 和设备交互
- 有极大的兼容问题
- 可以基于第三方平台解决`(但是就只能在这个平台上使用)`

    Android            SDK
    IOS                SDK
    微信                SDK
    支付宝              SDK
    基于第三方平台开发,那么就只能在这个平台上使用


- 流媒体

    获取摄像头(video)
    音频处理(audio)
    语音识别(audio)
    地理位置(经纬度,需要第三方插件平台,编译和反编译)
    电量、震动、网络环境(不同网络做不同的事情)
    重力感应和陀螺仪
    websocket

- 实际应用较少,因为兼容问题很大`(因为没有统一的标准,各家浏览器厂商的支持不一样,所以一般都使用的少)`

### 流媒体之`video`
- 开启摄像头
- - `(window)`.Navigator.getUserMedia( )
- - [MDN](https://developer.mozilla.org/zh-CN/docs/Web/API/Navigator/getUserMedia)
- - navigator.getUserMedia ( `constraints, successCallback, errorCallback` );

    Navigator.getUserMedia()方法提醒用户需要使用音频（0或者1）和（0或者1）视频输入设备，比如相机，屏幕共享，或者麦克风。
    //Navigator.getUserMedia( )兼容
    window.navigator.getUserMedia() = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
    //用户许可开启摄像头和音频设备,那么就执行successCallback;
    //该方法已被废弃
    navigator.getUserMedia = navigator.getUserMedia ||navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
	if (navigator.getUserMedia) {
	   navigator.getUserMedia({ audio: true, video: { width: 1280, height: 720 } },
	      function(stream) {
	         var video = document.querySelector('video');
	         video.src = window.URL.createObjectURL(stream);
	         video.onloadedmetadata = function(e) {
	           video.play();
	         };
	      },
	      function(err) {
	         console.log("The following error occurred: " + err.name);
	      }
	   );
	} else {
	   console.log("getUserMedia not supported");
	}

- 新的`mediaDevices`

     open.addEventListener('click',success)
     function success(){ // 返回媒体流信息
           // 老的浏览器可能根本没有实现 mediaDevices，所以我们可以先设置一个空的对象
           if (navigator.mediaDevices === undefined){
               navigator.mediaDevices = {};
           }
    // 一些浏览器部分支持 mediaDevices。我们不能直接给对象设置 getUserMedia
    // 因为这样可能会覆盖已有的属性。这里我们只会在没有getUserMedia属性的时候添加它。
           if (navigator.mediaDevices.getUserMedia===undefined) {
               navigator.mediaDevices.getUserMedia = function(constraints) {
    //首先，如果有getUserMedia的话，就获得它
                   var getUserMedia = navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
    // 一些浏览器根本没实现它 -那么就返回一个error到promise的reject来保持一个统一的接口
                   if (!getUserMedia) {
                       return Promise.reject(new Error('getUserMedia is not implemented in this browser'));
                   }
    // 否则，为老的navigator.getUserMedia方法包裹一个Promise
                   return new Promise(function(resolve, reject) {
                       getUserMedia.call(navigator, constraints, resolve, reject);
                   });
               }
           }
           navigator.mediaDevices.getUserMedia({ audio: false, video: true })
               .then(function(stream) {
                   buffer = stream;
                   // 旧的浏览器可能没有srcObject
                   if ("srcObject" in oVideo) {
                       oVideo.srcObject = buffer;
                   } else {
                       // 防止再新的浏览器里使用它，因为它已经不再支持了
                       oVideo.src = window.URL.createObjectURL(buffer);
                   }
                   oVideo.onloadedmetadata = function(e) {
                       oVideo.play();
                   };
               })
               .catch(function(err) {
                   console.log(err.name + ": " + err.message);
               });
       }
       // 关闭
       close.addEventListener('click',function(){
           console.log( buffer.getTracks());
           buffer.getTracks()[0].stop();
       })
       
### 流媒体之audio
- 1.[web Audio API][1]
- 2.[web Audio API][2]
-  Audio标签提供的只是音频文件的播放，而**Web Audio API**则是给了开发者在浏览器里对`音频数据进行分析、处理的接口`，比如**混音、音阶和和弦、采样、绘制时间域下的波形图、绘制频率域下的波谱图、解码和回放、滤波器、均衡、失真、读取、文本转换成语音、语音识别成文本**等
- **`制作音频对象`**

    1.创建一个音乐对象
        var actx = new (window.AudioContext || window.webkitAudioContext);
        \创建音频对象运行环境，才能使用对象提供的api
    2.创建分析机,加工处理节点
        var analyser = actx.createAnalyser( );
        \表示一个可以提供时时频率分析，和时间域分析的切点，这些分析数据可以用做数据分析和可视化
    3.创建媒体源
        var audioSrc = actx.createMediaElementSource(audio);
    4.将媒体源与分析机连接
        audioSrc.connect( analyser );
        mp3 -> 分析机 -> 耳机(目的地)
        \source → analyser → destination
    5.将分析机与目标点连接（扬声器）
        analyser.connect( actx.destination );
        \destination定义了最后音频要输出到哪里
    6.加载音乐资源二种方式
        静态的，加载自己服务器里的资源
        动态的，加载其它服务器里的资源 Ajax jsonP
    7.对音频节点分析
        利用大量滤波器BiquadFilterNode调整音色
        使用ChannelSplitterNode分割左右声道
        GainNode调整增益值实现音乐淡入淡出
    8.被渲染音频流输出到扬声器
    
    
- audio的一些标签特性
- - audio.volume = 0.1;//声音大小
- - audio.play();开始播放音乐
- - audio.pause();暂停音乐
-  `利用canvas绘图,并展示出来`

    1.创建一个与音频频次等长的数组
        var voicehigh =new Uint8Array(analyser.frequencyBinCount)
        利用这个然后动态改变每一个,就可以做出来该特效
        var output = new Uint8Array(analyser.fftSize)
        创建一个数组来存储数据，音乐在播放时，能即时获取对应的音频数据
        Uint8Array
            数组类型表示一个8位无符号整型数组，创建时内容被初始化为0。
        创建完后，可以以对象的方式或使用数组下标索引的方式引用数组中的元素。
        以fftSize为长度（默认长度2048）创建一个字节数组作为数据缓冲区
        fftSize 表示频谱分析下的快速傅里叶变换的大小，取值范围是 32-2048 之内 2 的整数次方，即 32、64、128、256、512、1024、2048。默认值为 2048。
    \
    var output=new Uint8Array(analyser.frequencyBinCount*44100/context.sampleRate)
    计算出采样比率44100所需的缓冲区长度
    \//可以不执行analyser.frequencyBinCount*44100/context.sampleRate计算,直接写analyser.frequencyBinCount
    frequencyBinCount 是fftSize 值的一半。这个值代表你要绘制的条形频谱图中有多少条条形。注意，这是个只读的属性，要修改这个属性的值请改 fftSize
        context.sampleRate 
        属性返回一个浮点数表示采样率（每秒采样数）， 
        同一个AudioContext中的所有节点采样率相同，所以不支持采样率转换
        analyser.getByteFrequencyData(output)每次调用，从分析机里获取即时时域音频图数据
    2.analyser.getByteTimeDomainData(output);   
        每次调用，从分析机里获取即时时域音波图数据
    3.将分析出来的音频数据添加到数组里面
        analyser.getByteFrequencyData(voicehigh);
        如果mp3 在播放 voicehigh就有数据,没有播放,那么返回undefined
        就以这个的动态改变就可以做出来自己想要的效果
    4.由于音频频次数据的长度1024，可以只取一部分
        var step = Math.round(voicehigh.length/100);
    5.绘制100个条形图 
        var iH = (voicehigh[step*i])/3; 是每一个的高度
        
- 示例: 

    <script>
        var audio = document.querySelector('audio');
        // 创建音频对象
        var oCxt = new AudioContext();
        // 创建分析机
        var analyser = oCxt.createAnalyser();

        // 创建媒体源
        var audioSrc = oCxt.createMediaElementSource(audio);

        audioSrc.connect(analyser);
        // mp3 -> 分析机 -> 目的地（耳机）
        audioSrc.connect(oCxt.destination);
        // 以上代码  那么可以实时监听 mp3 播放的状态
        var c = document.querySelector('canvas');
        var cxt = c.getContext('2d');
        var w = c.width = window.innerWidth;
        var h = c.height = window.innerHeight;
        window.addEventListener('resize',function(){
            c.width = window.innerWidth;
            c.height = window.innerHeight;
        })
        var color = cxt.createLinearGradient(w/2,h/2-100,w/2,h/2+100);
        color.addColorStop(0,'red');
        color.addColorStop(0.5,'green');
        color.addColorStop(1,'blue');
        //cxt.fillStyle = color;
        //cxt.fillRect(0,0,w,h)
        //1.创建一个与音频频次等长的数组
        var voicehigh = new Uint8Array(analyser.frequencyBinCount);
        var count = 200;
        function draw() {
            cxt.clearRect(0 ,0,w,h)
            // 2.将分析出来的音频数据添加到数组里面
            // 如果mp3 在播放 voicehigh就有数据
            analyser.getByteFrequencyData(voicehigh);
            // 函数默认返回undefiend
            var step =  Math.round(voicehigh.length/count);
            for( var i=0;i<count;i++ ){
                var iH = voicehigh[step*i];
                cxt.fillStyle = color;
                cxt.fillRect(i*5+w/2,h/2,4,-iH);
                cxt.fillRect(w/2-i*6,h/2,5,-iH);
                cxt.fillRect(i*5+w/2,h/2,4,iH);
                cxt.fillRect(w/2-i*6,h/2,5,iH);
            }
            requestAnimationFrame(draw);
        };


        audio.volume = 0.1;
        var onOff = true;
        document.addEventListener('click',function(){
            onOff?audio.play():audio.pause();
            onOff = !onOff;
            draw()
        })
    </script>


### 语音识别
- 先阶段只有谷歌移动端支持(**`好像`**);
- 

    var newRecognition= new webkitSpeechRecognition( );//实例化对象
    newRecognition.continuous = true;// 设置是持续听还是听到声音之后就关闭接收
    newRecognition.interimResults = true;// 是否返回结果newRecognition.start( ); 开启语音识别
    newRecognition.stop( );//	停止语音识别
    newRecognition.onresult = f( ){ //分析语音结果
        for (var i = e.resultIndex; i < e.results.length; ++i) {
            str += e.results[i][0].transcript;//渲染到页面
        }
    } 

- 示例

    <script>

        // 在移动端 谷歌浏览器 支持
        // 文字 转换成语音
        play.addEventListener('click',function(){
            var text = con.innerText;
            var msg = new SpeechSynthesisUtterance( text );
            window.speechSynthesis.speak(msg);
        })
        var newRecognition = new webkitSpeechRecognition();
        // 实例化对象
        newRecognition.continuous = true;
        // b)设置是持续听还是听到声音之后就关闭接收
        newRecognition.interimResults = true;//是否返回结果
        // 把语音 转换成文字
        start.addEventListener('click',function () {
            newRecognition.start( );  //开启语音识别
        })
        end.addEventListener('click',function(){
            newRecognition.stop( );	//停止语音识别
        })
        //录音
        newRecognition.onresult = function(e){ //分析语音结果
            var str = '';
            for(var i = e.resultIndex; i < e.results.length;++i){
                str += e.results[i][0].transcript;
            }
            txt.innerHTML = str;//写入页面
        }
    </script>
    
### xhr请求播放`audio`
- **`过程`**
1. 使用 XMLHTTPRequest 对象加载音乐资源数据
2. 加载完成后，音乐数据将被存储到 request.response 里
3. 使用 ctx.decodeAudioData 把音乐文件解码成 buffer（异步）


- 创建一个XMLHttpRequest请求对象

    var xhr = new XMLHttpRequest( );
    
- 请求一个MP3文件

       var request = new XMLHttpRequest();
        request.responseType = 'arraybuffer';
        //必须,否则下面的代码不起作用
        request.onload = function(){
            oCtx.decodeAudioData(//加载完成后解码
                request.response,
                function (buffer) {//buffer是该解码函数的值
                    console.log( 1 );
                    playBuffer(buffer);
                },function(){
                    //错误的回调函数(例如文件损坏、格式不对等)
                }
            );
        }
        request.open('POST','Dream.mp3',true);
        request.send();
- 播放音乐

    //播放声音
    function playBuffer(buffer) { 
      var sourceNode = ctx.createBufferSource( );
      sourceNode.buffer = buffer;
      sourceNode.connect(ctx.destination);
      sourceNode.start(0);
    }
    
- 代码示例

    <script>
        var oCtx = new ( window.AudioContext || window.webkitAudioContext )();
        var request = new XMLHttpRequest();
        request.responseType = 'arraybuffer';
        request.onload = function(){
            oCtx.decodeAudioData(
                request.response,
                function (buffer) {
                    console.log( 1 );
                    playBuffer(buffer);
                }
            );
        }
        request.open('POST','Dream.mp3',true);
        request.send();
        
        function playBuffer(buffer) {
            var sourceNode = oCtx.createBufferSource(); 
            // 创建音频源节点
            sourceNode.buffer = buffer;  // 赋值播放路径
            sourceNode.loop = true;
            sourceNode.loopStart = 10;    // 从10秒开始循环
            sourceNode.loopEnd = 15;    // 在第15秒结束
            var gainNode = oCtx.createGain();  // 创建 音量节点
            gainNode.gain.value = 0.3; // 音量  0~1
         //gainNode.gain.setValueAtTime(0.1,oCtx.currentTime+5);
        //gainNode.gain.setValueAtTime(0.3,oCtx.currentTime+8);
            sourceNode.connect(gainNode);
            gainNode.connect(oCtx.destination);
            //sourceNode.start(0,10,5); // 第一参数 延迟多少秒开始播放, 从第几秒开始播放, 要播放多少秒
            sourceNode.start();
            sourceNode.onended = function(){
                console.log( '结束了' );
            }
            // web audio api不能够 随时 暂停和播放,可以使用cookie的方式来做到暂停和播放
            setTimeout(function(){
                sourceNode.stop();
            },5000)
        }
    </script>
    
- 一些API

    var oCtx = new ( window.AudioContext || window.webkitAudioContext )();
    var obj = oCtx.createBufferSource();
    播放
    \obj.start();
    开始循环
    \obj.loop = true;
    \obj.loopStart = 10;//从哪儿开始(10)
    \obj.loopEnd = 20;//到哪儿停止(20)
    可以用一个API来代替
    \obj.start(delay,10,20);//延迟多少,从哪儿开始,持续时间
    监听音乐是否结束,结束触发
    obj.onended = function(){
      //这个也可以用定时器来代替(obj.stop())
    }
    音量节点
    \oCtx.createGain();
    音量大小
    \oCtx.createGain().value = 0.3;0-1
    音量自增(逐渐增强)
    \oCtx.createGain().gain.setValueAtTime(0.1,oCtx.currentTime+1,duration);//可以写多个   0.1是音量大小,第二个参数是当前多少(1)时间后开始
    
### 3D环绕音效
[3D环绕音效][3]
- PannerNode.setPosition(x, y, z) 设置声源空间位置 (x, y, z)
- - **`-------`** 一般改变这个**xyz**值即可实现
- PannerNode.setOrientation(x, y, z)设置声源的朝向 (x, y, z)。(x, y, z)是代表方向的向量值。
- PannerNode.setVelocity(x, y, z)设置声源的运动方向和速度 (x, y, z)。(x, y, z)是代表方向和速度的向量值。

### WebSocket
- 现在写这个还有点吃力,把视频保存起来,等到把node.js和Vue.js都学习了再来写这个的有关知识点`(20180525)`

### 横竖屏、重力感应器、陀螺仪、位置、电量
- 都在以后把它补上吧，现在我没学，也没有做笔记，只是现在学感觉有些不懂，也不是很有必要，加油(`20180527`)

  [1]: https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API
  [2]: https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Audio_API
  [3]: https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance